{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzsYar8Oujiq"
   },
   "source": [
    "###### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gVjC6UH-uidr",
    "outputId": "8ace0229-bcc5-4772-f4a7-f7be771a47d8"
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), '../src'))\n",
    "sys.path.append(os.path.join(os.getcwd(), './data'))\n",
    "import csv\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [20, 6]\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "\n",
    "from arch import arch_model\n",
    "\n",
    "from MultiValidPrediction import MultiValidPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdUuqeZmvk9a"
   },
   "outputs": [],
   "source": [
    "def stock_history(name, datefrom='01/03/00', dateto='12/31/20', ret_scale=100):\n",
    "  file = open('.\\data\\\\' + name + '.csv')\n",
    "  csv_file = csv.reader(file)\n",
    "  header= next(csv_file)\n",
    "\n",
    "  rows = []\n",
    "  for row in csv_file:\n",
    "          rows.append(row)\n",
    "\n",
    "  rows = np.array(rows)\n",
    "\n",
    "  dates = np.array(rows[:, 0])\n",
    "  open_prices = np.array([float(price) for price in rows[:, 1]])\n",
    "\n",
    "  begin = np.where(dates==datefrom)[0][0]\n",
    "\n",
    "  end = np.where(dates==dateto)[0][0]\n",
    "\n",
    "  prices = open_prices[end:begin][::-1]\n",
    "\n",
    "  T = len(prices)\n",
    "\n",
    "  returns = [(prices[1]/prices[0]-1)]\n",
    "  for t in range(1, T):\n",
    "    returns.append(prices[t]/prices[t-1] - 1)\n",
    "  \n",
    "  returns = [ret * ret_scale for ret in returns] # scale returns\n",
    "\n",
    "  volatility = [ret**2 for ret in returns]\n",
    "\n",
    "  f, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=[15, 12], sharex=True)\n",
    "  ax1.plot(prices)\n",
    "  ax1.set_ylabel(name + ' prices')\n",
    "  ax2.plot(returns)\n",
    "  ax2.set_ylabel(name + ' returns')\n",
    "  ax3.plot(volatility)\n",
    "  ax3.set_ylabel(name + ' volatility')\n",
    "  f.suptitle(name + ' Historical Data', fontsize=20)\n",
    "  f.show()\n",
    "  \n",
    "  return T, prices, returns, volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AZLZ8PyOvo4c"
   },
   "outputs": [],
   "source": [
    "def garch_scores(returns, volatility, lookback=100, offset=10, score_type='regr', score_unit_interval=True, norm_const=-1):\n",
    "  # initialize prediction model\n",
    "  garch_model = arch_model(returns, vol='Garch', p=1, q=1)\n",
    "\n",
    "  predictions = [0 for i in range(offset)]\n",
    "  scores = [0 for i in range(offset)]\n",
    "\n",
    "  for t in range(offset, len(volatility)):\n",
    "      # current window indices\n",
    "      left_ind = max(0, t-lookback)\n",
    "      right_ind = t\n",
    "\n",
    "      # compute score\n",
    "      variance_pred_array = garch_model.fit(first_obs=left_ind, last_obs=right_ind, disp='off', show_warning=False).forecast(reindex=False).variance\n",
    "      varNext = variance_pred_array.iloc[0]['h.1'] #['h.1'][lookback-1]\n",
    "      score = score_fn(actual=volatility[t], pred=varNext, score_type=score_type, unit_interval_norm=score_unit_interval, divide_by_const=(norm_const != -1), norm_const=norm_const)\n",
    "\n",
    "      # update arrays with data\n",
    "      scores.append(score)\n",
    "      predictions.append(varNext)\n",
    "\n",
    "  return scores, predictions\n",
    "\n",
    "def score_fn(actual, pred, score_type, unit_interval_norm=False, divide_by_const=False, norm_const=1000):\n",
    "  # what kind of score?\n",
    "  if score_type=='regr':\n",
    "    scr = abs(actual-pred)\n",
    "  if score_type=='regr_normalized':\n",
    "    scr = abs(actual-pred)/pred\n",
    "  \n",
    "  # normalize score into [0, 1]\n",
    "  if unit_interval_norm: \n",
    "    if divide_by_const:\n",
    "      scr /= norm_const\n",
    "    else:\n",
    "      scr = scr/(1+scr)\n",
    "  return scr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6kDqK8hExbrs"
   },
   "outputs": [],
   "source": [
    "def ACI_method(scores, alpha=0.1, lookback=100, offset=10, gamma=0.005):\n",
    "  T = len(scores)\n",
    "\n",
    "  # initialize ACI\n",
    "  alphat = alpha\n",
    "\n",
    "  alphas = [alpha for i in range(offset)]\n",
    "  thresholds = [0 for i in range(offset)]\n",
    "  err_seq = [0 for i in range(offset)]\n",
    "\n",
    "  for t in range(offset, T):\n",
    "      # current scoring window\n",
    "      left_ind = max(0, t-lookback)\n",
    "      right_ind = t\n",
    "      recent_scores = scores[left_ind: right_ind]\n",
    "\n",
    "      if 1 - alphat > 1:\n",
    "        threshold = 1\n",
    "      elif 1 - alphat < 0:\n",
    "        threshold = 0\n",
    "      else:\n",
    "        threshold = np.quantile(recent_scores, 1-alphat)\n",
    "      err_ind = int(scores[t] > threshold)\n",
    "\n",
    "      # ACI alphat update\n",
    "      alphat = alphat + gamma*(alpha-err_ind)\n",
    "\n",
    "      # update arrays with data\n",
    "      alphas.append(alphat)\n",
    "      thresholds.append(threshold)\n",
    "      err_seq.append(err_ind)\n",
    "  \n",
    "  miscoverage_rate_ACI = np.mean(np.array(err_seq))\n",
    "  print('ACI miscoverage rate: ', miscoverage_rate_ACI)\n",
    "\n",
    "  return thresholds, alphas, miscoverage_rate_ACI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDuyI7HpxpD5"
   },
   "source": [
    "###### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 787
    },
    "id": "v4AQh3P8xuB9",
    "outputId": "fb51a15e-4bd5-40eb-9949-824350503656"
   },
   "outputs": [],
   "source": [
    "T, prices, returns, volatility = stock_history('AMD', ret_scale=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZbIY-H-x8Xi"
   },
   "outputs": [],
   "source": [
    "# CREATE NOISY DATA FOR MULTIGROUP EXPERIMENT\n",
    "\n",
    "num_groups = 20\n",
    "\n",
    "returns_noisy = np.zeros(len(returns))\n",
    "std_returns = np.std(returns)\n",
    "for t in range(len(returns)):\n",
    "  returns_noisy[t] = returns[t]\n",
    "  for j in range(1, num_groups+1):\n",
    "    if t % j == 0:\n",
    "      returns_noisy[t] += std_returns*np.random.randn()\n",
    "\n",
    "volatility_noisy = [ret**2 for ret in returns_noisy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xC6OFeX5yQ43"
   },
   "outputs": [],
   "source": [
    "scores_noisy, predictions_noisy = garch_scores(returns_noisy, volatility_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDfysDbMysvE"
   },
   "outputs": [],
   "source": [
    "scores, predictions = garch_scores(returns, volatility)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNNjqUjlzRD3"
   },
   "source": [
    "###### Experiment: Noisy Multigroup Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QtyqjdJrzfgJ",
    "outputId": "b2f25111-0c7b-4530-d219-bc2d1a165858"
   },
   "outputs": [],
   "source": [
    "# ACI Analysis\n",
    "\n",
    "ACI_thrs, ACI_alphas, ACI_miscoverage_rate = ACI_method(scores_noisy, alpha=0.1, lookback=100, offset=10, gamma=0.005)\n",
    "\n",
    "score_seq = scores_noisy\n",
    "thrs_seq = ACI_thrs\n",
    "print_ACI_group_stats = False\n",
    "\n",
    "g_miscoverage = np.zeros(num_groups+1, dtype=int)\n",
    "g_counts = np.zeros(num_groups+1, dtype=int)\n",
    "\n",
    "g_counts_residual = 0\n",
    "g_miscoverage_residual = 0\n",
    "g_counts_div_by_smth = 0\n",
    "g_miscoverage_div_by_smth = 0\n",
    "\n",
    "for t in range(10, len(thrs_seq)):\n",
    "  divisible_by_something = False\n",
    "  for j in range(1, num_groups+1):\n",
    "    if t % j == 0:\n",
    "      g_counts[j] += 1\n",
    "      g_miscoverage[j] += int(score_seq[t] > thrs_seq[t])\n",
    "      if j > 1:\n",
    "        divisible_by_something = True\n",
    "  if not divisible_by_something:\n",
    "    g_counts_residual += 1\n",
    "    g_miscoverage_residual += int(score_seq[t] > thrs_seq[t])\n",
    "  else: \n",
    "    g_counts_div_by_smth += 1\n",
    "    g_miscoverage_div_by_smth += int(score_seq[t] > thrs_seq[t])\n",
    "\n",
    "ACI_coverage_rate = 1-np.array([g_miscoverage[j]/g_counts[j] for j in range(1, num_groups+1)])\n",
    "\n",
    "if print_ACI_group_stats:\n",
    "    print('Per-Group Coverage Statistics:')\n",
    "\n",
    "    for j in range(num_groups-1):\n",
    "      print('Group ', j+1, ' : Coverage: ', ACI_coverage_rate[j])\n",
    "\n",
    "    print(1 - g_miscoverage_residual/g_counts_residual)\n",
    "    print(g_miscoverage_residual)\n",
    "    print(g_counts_residual)\n",
    "\n",
    "    print(1 - g_miscoverage_div_by_smth/g_counts_div_by_smth)\n",
    "    print(g_miscoverage_div_by_smth)\n",
    "    print(g_counts_div_by_smth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKEOILDE0Nh_"
   },
   "outputs": [],
   "source": [
    "# MVP\n",
    "\n",
    "groups = []\n",
    "for i in range(1, num_groups + 1):\n",
    "    group = (lambda t, i=i: (t % i) == 0)\n",
    "    groups.append(group)\n",
    "\n",
    "n_buckets = 40\n",
    "T = len(scores_noisy)\n",
    "eta = np.sqrt(np.log(len(groups) * n_buckets) / (2 * len(groups) * n_buckets))\n",
    "\n",
    "MVP_group = MultiValidPrediction(n_buckets=n_buckets, groups=groups, eta=eta)\n",
    "\n",
    "for t in range(T):\n",
    "    x_t = t\n",
    "    score = scores_noisy[t]\n",
    "\n",
    "    thr = MVP_group.predict(x_t)\n",
    "    MVP_group.update(x_t, thr, score)\n",
    "\n",
    "thresholds_MVP_group, miscoverage_MVP_group, counts_MVP_group = MVP_group.coverage_stats(plot_thresholds=False, print_per_group_stats=False)\n",
    "\n",
    "MVP_coverage_rate = 1-np.array([np.sum(miscoverage_MVP_group[:, j])/np.sum(counts_MVP_group[:, j]) for j in range(num_groups)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "qeNfyLi44Gw4",
    "outputId": "77d60510-6748-490b-ab93-a54ce4af1534"
   },
   "outputs": [],
   "source": [
    "barWidth = 0.25\n",
    "delta = 0.1\n",
    "br1 = np.arange(num_groups)\n",
    "br2 = [x + barWidth for x in br1]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.bar(br1, ACI_coverage_rate, color = 'm', width = barWidth, edgecolor = 'gray', label = 'ACI')\n",
    "plt.bar(br2, MVP_coverage_rate, color = 'c', width = barWidth, edgecolor = 'gray', label = 'MVP')\n",
    "group_labels = [str(i) for i in range(1, num_groups+1)]\n",
    "plt.xticks([r + barWidth for r in range(num_groups)], group_labels)\n",
    "plt.axhline(y= 1 - delta, c = 'r', linewidth = 0.5)\n",
    "plt.text(14, 1 - delta + 0.02, '  desired coverage')\n",
    "plt.legend()\n",
    "plt.ylim([0.0,1.2])\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.xlabel('Coverage on Noisy Groups: MVP vs ACI')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ovCmvrv4hMQ"
   },
   "source": [
    "###### Experiment: Sorted Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "EFfQDYtg4kYo",
    "outputId": "06d14679-bdf2-4812-ebd6-2e44974e2774"
   },
   "outputs": [],
   "source": [
    "synthetic_scores = np.linspace(0, 0.5, num=len(prices))\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(synthetic_scores)\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Sorted scores fed to MVP and ACI')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "id": "xdq_2yke4y_2",
    "outputId": "4de6398e-2042-49c2-b447-75fe34e990c7"
   },
   "outputs": [],
   "source": [
    "# ACI\n",
    "\n",
    "ACI_thresholds_sorted, ACI_alphas_sorted, ACI_miscoverage_rate_sorted = ACI_method(synthetic_scores)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.hist(ACI_thresholds_sorted, bins = 40)\n",
    "plt.xlabel('Threshold values (binned)')\n",
    "plt.ylabel('No. times threshold value is predicted by ACI')\n",
    "plt.title('Histogram of ACI thresholds on sorted data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "8zLHrjVy4-3N",
    "outputId": "7b280a88-f89d-43f0-9b5e-805f61dacda6"
   },
   "outputs": [],
   "source": [
    "# MVP\n",
    "\n",
    "n_buckets = 40\n",
    "T = len(synthetic_scores)\n",
    "eta_sorted = np.sqrt(np.log(2 * 1 * n_buckets) / T)\n",
    "\n",
    "MVP_sorted = MultiValidPrediction(n_buckets=n_buckets, eta=eta_sorted, normalize_by_counts=False)\n",
    "\n",
    "for t in range(T):\n",
    "    x_t = t\n",
    "    score_sorted = synthetic_scores[t]\n",
    "\n",
    "    thr_sorted = MVP_sorted.predict(x_t)\n",
    "    MVP_sorted.update(x_t, thr_sorted, score_sorted)\n",
    "\n",
    "thresholds_MVP_sorted, miscoverage_MVP_sorted, counts_MVP_sorted = MVP_sorted.coverage_stats(plot_thresholds=True, print_per_group_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ysCNO0t5oWV",
    "outputId": "43f1821e-34a4-4f61-fe29-ee9592843324"
   },
   "outputs": [],
   "source": [
    "print('Comparing predicted widths of ACI vs MVP:')\n",
    "\n",
    "print('ACI widths: ', np.mean(np.array(ACI_thresholds_sorted)))\n",
    "print('MVP widths: ', np.mean(np.array(thresholds_MVP_sorted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKJ0Mk0G6dBM"
   },
   "source": [
    "###### Experiment: Marginal coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x76C0zlk6lCj",
    "outputId": "671e5128-f610-4de7-e47e-26ac467f7289"
   },
   "outputs": [],
   "source": [
    "ACI_thrs_single, ACI_alphas_single, ACI_miscoverage_rate_single = ACI_method(scores, alpha=0.1, lookback=100, offset=10, gamma=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "HdNsXk1H7mAG",
    "outputId": "85047ab1-2c6c-4bbc-fe9c-7fde3fb37a8a"
   },
   "outputs": [],
   "source": [
    "# MVP\n",
    "\n",
    "n_buckets = 40\n",
    "T = len(scores)\n",
    "eta = np.sqrt(np.log(2 * 1 * n_buckets) / T)\n",
    "\n",
    "MVP_single = MultiValidPrediction(n_buckets=n_buckets, eta=eta, normalize_by_counts=False)\n",
    "\n",
    "for t in range(T):\n",
    "    x_t = t\n",
    "    score = scores[t]\n",
    "\n",
    "    thr = MVP_single.predict(x_t)\n",
    "    MVP_single.update(x_t, thr, score)\n",
    "\n",
    "thresholds_MVP_single, miscoverage_MVP_single, counts_MVP_single = MVP_single.coverage_stats(plot_thresholds=True, print_per_group_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "cnLLngpK8XBg",
    "outputId": "39d6be2a-22e5-4899-999d-b99b3b389219"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(ACI_thrs_single, label='ACI')\n",
    "plt.plot(thresholds_MVP_single, label='MVP')\n",
    "plt.legend(loc='lower right', shadow=True, fontsize='x-large')\n",
    "plt.ylim([0.8, 1.02])\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Thresholds')\n",
    "plt.title('MVP vs ACI thresholds, AMD stock data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Multigroup error bars for the Noisy Multigroup Coverage experiment (takes a long time to run due to trial number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_groups = 20\n",
    "\n",
    "scores_arr = []\n",
    "\n",
    "for trial in range(20):\n",
    "    returns_noisy = np.zeros(len(returns))\n",
    "    std_returns = np.std(returns)\n",
    "    for t in range(len(returns)):\n",
    "        returns_noisy[t] = returns[t]\n",
    "        for j in range(1, num_groups+1):\n",
    "            if t % j == 0:\n",
    "                returns_noisy[t] += std_returns*np.random.randn()\n",
    "\n",
    "    volatility_noisy = [ret**2 for ret in returns_noisy]\n",
    "    scores_noisy, predictions_noisy = garch_scores(volatility_noisy)\n",
    "\n",
    "    scores_arr.append(scores_noisy)\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle_out = open('noisyscores0.pickle', 'wb')\n",
    "pickle.dump(scores_arr, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "group_cvgs_ACI = []\n",
    "group_cvgs_MVP = []\n",
    "\n",
    "groups = []\n",
    "for i in range(1, num_groups + 1):\n",
    "    group = (lambda t, i=i: (t % i) == 0)\n",
    "    groups.append(group)\n",
    "\n",
    "for trial in range(20):\n",
    "    scores_curr = scores_arr[trial]\n",
    "\n",
    "    # ACI\n",
    "    ACI_thrs, ACI_alphas, ACI_miscoverage_rate = ACI_method(scores_curr, alpha=0.1, lookback=100, offset=10, gamma=0.005)\n",
    "\n",
    "    # ACI Coverage rates\n",
    "    g_miscoverage = np.zeros(num_groups+1, dtype=int)\n",
    "    g_counts = np.zeros(num_groups+1, dtype=int)\n",
    "\n",
    "    thrs_seq = ACI_thrs\n",
    "    for t in range(10, len(thrs_seq)):\n",
    "        for j in range(1, num_groups+1):\n",
    "            if t % j == 0:\n",
    "                g_counts[j] += 1\n",
    "                g_miscoverage[j] += int(scores_curr[t] > thrs_seq[t])\n",
    "\n",
    "    ACI_coverage_rate = 1-np.array([g_miscoverage[j]/g_counts[j] for j in range(1, num_groups+1)])\n",
    "\n",
    "    # MVP\n",
    "    n_buckets = 40\n",
    "    T = len(scores_curr)\n",
    "    eta = np.sqrt(np.log(2 * len(groups) * n_buckets) / T)\n",
    "    MVP_group = MultivalidPredictionIntervals(n_buckets=n_buckets, groups=groups, eta=eta)\n",
    "    for t in range(T):\n",
    "        x_t = t\n",
    "        thr = MVP_group.predict(x_t)\n",
    "        score = scores_curr[t]\n",
    "        MVP_group.update(x_t, thr, score)\n",
    "\n",
    "    # MVP Coverage rates\n",
    "    thresholds_MVP_group, miscoverage_MVP_group, counts_MVP_group = MVP_group.coverage_stats(plot_thresholds=False, print_per_group_stats=False)\n",
    "    MVP_coverage_rate = 1-np.array([np.sum(miscoverage_MVP_group[:, j])/np.sum(counts_MVP_group[:, j]) for j in range(num_groups)])\n",
    "\n",
    "    # Add ACI and MVP coverage to array\n",
    "    group_cvgs_ACI.append(ACI_coverage_rate)\n",
    "    group_cvgs_MVP.append(MVP_coverage_rate)\n",
    "\n",
    "cvgs_ACI = np.array(group_cvgs_ACI)\n",
    "cvgs_MVP = np.array(group_cvgs_MVP)\n",
    "\n",
    "# Error bars and medians\n",
    "ACI_err_upper = [np.quantile(cvgs_ACI[:,group], 0.75) for group in range(num_groups)]\n",
    "MVP_err_upper = [np.quantile(cvgs_MVP[:,group], 0.75) for group in range(num_groups)]\n",
    "\n",
    "ACI_err_lower = [np.quantile(cvgs_ACI[:,group], 0.25) for group in range(num_groups)]\n",
    "MVP_err_lower = [np.quantile(cvgs_MVP[:,group], 0.25) for group in range(num_groups)]\n",
    "\n",
    "ACI_median    = [np.quantile(cvgs_ACI[:,group], 0.50) for group in range(num_groups)]\n",
    "MVP_median    = [np.quantile(cvgs_MVP[:,group], 0.50) for group in range(num_groups)]\n",
    "\n",
    "# Histogram\n",
    "\n",
    "barWidth = 0.25\n",
    "delta = 0.1\n",
    "br1 = np.arange(num_groups)\n",
    "br2 = [x + barWidth for x in br1]\n",
    "br3 = [x + barWidth for x in br2]\n",
    "\n",
    "plt.bar(br1, ACI_median, color = 'm', width = barWidth, edgecolor = 'gray', label = 'ACI')\n",
    "plt.bar(br2, MVP_median, color = 'c', width = barWidth, edgecolor = 'gray', label = 'MVP')\n",
    "\n",
    "for i, val in enumerate(br1):\n",
    "    plt.vlines(x=val, ymin=ACI_err_lower[i], ymax=ACI_err_upper[i], color='black', linewidth=2)\n",
    "\n",
    "for i, val in enumerate(br2):\n",
    "    plt.vlines(x=val, ymin=MVP_err_lower[i], ymax=MVP_err_upper[i], color='black', linewidth=2)\n",
    "\n",
    "group_labels = [str(i) for i in range(1, num_groups+1)]\n",
    "plt.xticks([r + barWidth for r in range(num_groups)], group_labels)\n",
    "plt.axhline(y= 1 - delta, c = 'r', linewidth = 0.5)\n",
    "plt.text(14, 1 - delta + 0.02, '  desired coverage')\n",
    "plt.legend()\n",
    "plt.ylim([0.0,1.2])\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.title('Coverage on Noisy Groups: MVP vs ACI')\n",
    "plt.xlabel('Group number')\n",
    "plt.ylabel('Coverage level')\n",
    "plt.show()\n",
    "\n",
    "# print maximum error bar width for ACI and the corresponding group\n",
    "maxdist = 0\n",
    "gr = -1\n",
    "for group in range(num_groups):\n",
    "    a = ACI_err_lower[group] \n",
    "    b = ACI_median[group]\n",
    "    c = ACI_err_upper[group]\n",
    "    if abs(a-c) > maxdist:\n",
    "        maxdist = abs(a-c)\n",
    "        gr = group\n",
    "print(maxdist)\n",
    "print(gr)\n",
    "\n",
    "# print maximum error bar width for MVP and the corresponding group\n",
    "maxdist = 0\n",
    "gr = -1\n",
    "for group in range(num_groups):\n",
    "    a = MVP_err_lower[group] \n",
    "    b = MVP_median[group]\n",
    "    c = MVP_err_upper[group]\n",
    "    if abs(a-c) > maxdist:\n",
    "        maxdist = abs(a-c)\n",
    "        gr = group\n",
    "print(maxdist)\n",
    "print(gr)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "TimeSeriesExperiment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
